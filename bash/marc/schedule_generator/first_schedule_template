#ifndef COMPAT
#include <xen/config.h>
#include <xen/init.h>
#include <xen/lib.h>
#include <xen/sched.h>
#include <xen/domain.h>
#include <xen/delay.h>
#include <xen/event.h>
#include <xen/time.h>
#include <xen/timer.h>
#include <xen/perfc.h>
#include <xen/sched-if.h>
#include <xen/softirq.h>
#include <xen/trace.h>
#include <xen/mm.h>
#include <xen/err.h>
#include <xen/guest_access.h>
#include <xen/hypercall.h>
#include <xen/multicall.h>
#include <xen/cpu.h>
#include <xen/preempt.h>
#include <xen/event.h>
#include <public/sched.h>
#include <xsm/xsm.h>

/* opt_sched: scheduler - default to credit */
static char __initdata opt_sched[10] = "credit";
string_param("sched", opt_sched);

/* if sched_smt_power_savings is set,
 * scheduler will give preferrence to partially idle package compared to
 * the full idle package, when picking pCPU to schedule vCPU.
 */
bool_t sched_smt_power_savings = 0;
boolean_param("sched_smt_power_savings", sched_smt_power_savings);

/* Default scheduling rate limit: 1ms 
 * The behavior when sched_ratelimit_us is greater than sched_credit_tslice_ms is undefined
 * */
int sched_ratelimit_us = SCHED_DEFAULT_RATELIMIT_US;
integer_param("sched_ratelimit_us", sched_ratelimit_us);
/* Various timer handlers. */
static void s_timer_fn(void *unused);
static void vcpu_periodic_timer_fn(void *data);
static void vcpu_singleshot_timer_fn(void *data);
static void poll_timer_fn(void *data);

/* This is global for now so that private implementations can reach it */
DEFINE_PER_CPU(struct schedule_data, schedule_data);
DEFINE_PER_CPU(struct scheduler *, scheduler);

static const struct scheduler *schedulers[] = {
    &sched_sedf_def,
    &sched_credit_def,
    &sched_credit2_def,
    &sched_arinc653_def,
};

static struct scheduler __read_mostly ops;

#define SCHED_OP(opsptr, fn, ...)                                          \
         (( (opsptr)->fn != NULL ) ? (opsptr)->fn(opsptr, ##__VA_ARGS__ )  \
          : (typeof((opsptr)->fn(opsptr, ##__VA_ARGS__)))0 )

#define DOM2OP(_d)    (((_d)->cpupool == NULL) ? &ops : ((_d)->cpupool->sched))
#define VCPU2OP(_v)   (DOM2OP((_v)->domain))
#define VCPU2ONLINE(_v) cpupool_online_cpumask((_v)->domain->cpupool)




/* XEMPOWER_TAG */

/* Comment this to disable debug */
#define _XEMPOWER_DEBUG

#ifndef CUSTOM_LOGT
    #ifdef _XEMPOWER_DEBUG      
        #define CUSTOM_LOGT(fmt, ...)                                                \
        do {/** Prints log message with time. */                                     \
            /*console_start_sync();*/                                                \
            printk("[CUSTOM_LOGT @ %s() on cpu %d @ t = %ld us] " fmt,               \
                   __FUNCTION__, smp_processor_id(), get_s_time(), ##__VA_ARGS__);   \
            /*console_end_sync();*/                                                  \
        } while(0)
    #else
        #define CUSTOM_LOGT(fmt, ...)
    #endif
#else
    #error "Macro 'CUSTOM_LOGT' is already defined."
#endif




/* TODO - BETTER: use wrmsr and rdmsr function, as done in: xen/arch/x86/cpu/intel.c */
/* Read/write operations */
static long long read_msr(unsigned int ecx) {
    unsigned int edx = 0, eax = 0;
    unsigned long long result = 0;
    __asm__ __volatile__("rdmsr" : "=a"(eax), "=d"(edx) : "c"(ecx));
    result = eax | (unsigned long long)edx << 0x20;
    // CUSTOM_LOGT("Module msrdrv: Read 0x%016llx (0x%08x:0x%08x) from MSR 0x%08x\n", result, edx, eax, ecx);
    return result;
}

static void write_msr(int ecx, unsigned int eax, unsigned int edx) {
    // CUSTOM_LOGT("Module msrdrv: Writing 0x%08x:0x%08x to MSR 0x%04x\n", edx, eax, ecx);
    __asm__ __volatile__("wrmsr" : : "c"(ecx), "a"(eax), "d"(edx));
}

/* XEMPOWER_TAG */




static inline void trace_runstate_change(struct vcpu *v, int new_state)
{
    struct { uint32_t vcpu:16, domain:16; } d;
    uint32_t event;

    if ( likely(!tb_init_done) )
        return;

    d.vcpu = v->vcpu_id;
    d.domain = v->domain->domain_id;

    event = TRC_SCHED_RUNSTATE_CHANGE;
    event |= ( v->runstate.state & 0x3 ) << 8;
    event |= ( new_state & 0x3 ) << 4;

    __trace_var(event, 1/*tsc*/, sizeof(d), &d);
}

static inline void trace_continue_running(struct vcpu *v)
{
    struct { uint32_t vcpu:16, domain:16; } d;

    if ( likely(!tb_init_done) )
        return;

    d.vcpu = v->vcpu_id;
    d.domain = v->domain->domain_id;

    __trace_var(TRC_SCHED_CONTINUE_RUNNING, 1/*tsc*/, sizeof(d), &d);
}

static inline void vcpu_urgent_count_update(struct vcpu *v)
{
    if ( is_idle_vcpu(v) )
        return;

    if ( unlikely(v->is_urgent) )
    {
        if ( !test_bit(_VPF_blocked, &v->pause_flags) ||
             !test_bit(v->vcpu_id, v->domain->poll_mask) )
        {
            v->is_urgent = 0;
            atomic_dec(&per_cpu(schedule_data,v->processor).urgent_count);
        }
    }
    else
    {
        if ( unlikely(test_bit(_VPF_blocked, &v->pause_flags) &&
                      test_bit(v->vcpu_id, v->domain->poll_mask)) )
        {
            v->is_urgent = 1;
            atomic_inc(&per_cpu(schedule_data,v->processor).urgent_count);
        }
    }
}

static inline void vcpu_runstate_change(
    struct vcpu *v, int new_state, s_time_t new_entry_time)
{
    s_time_t delta;

    ASSERT(v->runstate.state != new_state);
    ASSERT(spin_is_locked(per_cpu(schedule_data,v->processor).schedule_lock));

    vcpu_urgent_count_update(v);

    trace_runstate_change(v, new_state);

    delta = new_entry_time - v->runstate.state_entry_time;
    if ( delta > 0 )
    {
        v->runstate.time[v->runstate.state] += delta;
        v->runstate.state_entry_time = new_entry_time;
    }

    v->runstate.state = new_state;
}

void vcpu_runstate_get(struct vcpu *v, struct vcpu_runstate_info *runstate)
{
    spinlock_t *lock = likely(v == current) ? NULL : vcpu_schedule_lock_irq(v);
    s_time_t delta;

    memcpy(runstate, &v->runstate, sizeof(*runstate));
    delta = NOW() - runstate->state_entry_time;
    if ( delta > 0 )
        runstate->time[runstate->state] += delta;

    if ( unlikely(lock != NULL) )
        vcpu_schedule_unlock_irq(lock, v);
}

uint64_t get_cpu_idle_time(unsigned int cpu)
{
    struct vcpu_runstate_info state = { 0 };
    struct vcpu *v = idle_vcpu[cpu];

    if ( cpu_online(cpu) && v )
        vcpu_runstate_get(v, &state);

    return state.time[RUNSTATE_running];
}

int sched_init_vcpu(struct vcpu *v, unsigned int processor) 
{
    struct domain *d = v->domain;

    /*
     * Initialize processor and affinity settings. The idler, and potentially
     * domain-0 VCPUs, are pinned onto their respective physical CPUs.
     */
    v->processor = processor;
    if ( is_idle_domain(d) || d->is_pinned )
        cpumask_copy(v->cpu_affinity, cpumask_of(processor));
    else
        cpumask_setall(v->cpu_affinity);

    /* Initialise the per-vcpu timers. */
    init_timer(&v->periodic_timer, vcpu_periodic_timer_fn,
               v, v->processor);
    init_timer(&v->singleshot_timer, vcpu_singleshot_timer_fn,
               v, v->processor);
    init_timer(&v->poll_timer, poll_timer_fn,
               v, v->processor);

    /* Idle VCPUs are scheduled immediately. */
    if ( is_idle_domain(d) )
    {
        per_cpu(schedule_data, v->processor).curr = v;
        v->is_running = 1;
    }

    TRACE_2D(TRC_SCHED_DOM_ADD, v->domain->domain_id, v->vcpu_id);

    v->sched_priv = SCHED_OP(DOM2OP(d), alloc_vdata, v, d->sched_priv);
    if ( v->sched_priv == NULL )
        return 1;

    SCHED_OP(DOM2OP(d), insert_vcpu, v);

    return 0;
}

int sched_move_domain(struct domain *d, struct cpupool *c)
{
    struct vcpu *v;
    unsigned int new_p;
    void **vcpu_priv;
    void *domdata;
    void *vcpudata;
    struct scheduler *old_ops;
    void *old_domdata;

    domdata = SCHED_OP(c->sched, alloc_domdata, d);
    if ( domdata == NULL )
        return -ENOMEM;

    vcpu_priv = xzalloc_array(void *, d->max_vcpus);
    if ( vcpu_priv == NULL )
    {
        SCHED_OP(c->sched, free_domdata, domdata);
        return -ENOMEM;
    }

    for_each_vcpu ( d, v )
    {
        vcpu_priv[v->vcpu_id] = SCHED_OP(c->sched, alloc_vdata, v, domdata);
        if ( vcpu_priv[v->vcpu_id] == NULL )
        {
            for_each_vcpu ( d, v )
            {
                if ( vcpu_priv[v->vcpu_id] != NULL )
                    xfree(vcpu_priv[v->vcpu_id]);
            }
            xfree(vcpu_priv);
            SCHED_OP(c->sched, free_domdata, domdata);
            return -ENOMEM;
        }
    }

    domain_pause(d);

    old_ops = DOM2OP(d);
    old_domdata = d->sched_priv;

    for_each_vcpu ( d, v )
    {
        SCHED_OP(old_ops, remove_vcpu, v);
    }

    d->cpupool = c;
    d->sched_priv = domdata;

    new_p = cpumask_first(c->cpu_valid);
    for_each_vcpu ( d, v )
    {
        spinlock_t *lock;

        vcpudata = v->sched_priv;

        migrate_timer(&v->periodic_timer, new_p);
        migrate_timer(&v->singleshot_timer, new_p);
        migrate_timer(&v->poll_timer, new_p);

        cpumask_setall(v->cpu_affinity);

        lock = vcpu_schedule_lock_irq(v);
        v->processor = new_p;
        /*
         * With v->processor modified we must not
         * - make any further changes assuming we hold the scheduler lock,
         * - use vcpu_schedule_unlock_irq().
         */
        spin_unlock_irq(lock);

        v->sched_priv = vcpu_priv[v->vcpu_id];
        if ( !d->is_dying )
            evtchn_move_pirqs(v);

        new_p = cpumask_cycle(new_p, c->cpu_valid);

        SCHED_OP(c->sched, insert_vcpu, v);

        SCHED_OP(old_ops, free_vdata, vcpudata);
    }

    domain_update_node_affinity(d);

    domain_unpause(d);

    SCHED_OP(old_ops, free_domdata, old_domdata);

    xfree(vcpu_priv);

    return 0;
}

void sched_destroy_vcpu(struct vcpu *v)
{
    kill_timer(&v->periodic_timer);
    kill_timer(&v->singleshot_timer);
    kill_timer(&v->poll_timer);
    if ( test_and_clear_bool(v->is_urgent) )
        atomic_dec(&per_cpu(schedule_data, v->processor).urgent_count);
    SCHED_OP(VCPU2OP(v), remove_vcpu, v);
    SCHED_OP(VCPU2OP(v), free_vdata, v->sched_priv);
}

int sched_init_domain(struct domain *d)
{
    SCHED_STAT_CRANK(dom_init);
    return SCHED_OP(DOM2OP(d), init_domain, d);
}

void sched_destroy_domain(struct domain *d)
{
    SCHED_STAT_CRANK(dom_destroy);
    SCHED_OP(DOM2OP(d), destroy_domain, d);
}

void vcpu_sleep_nosync(struct vcpu *v)
{
    unsigned long flags;
    spinlock_t *lock = vcpu_schedule_lock_irqsave(v, &flags);

    if ( likely(!vcpu_runnable(v)) )
    {
        if ( v->runstate.state == RUNSTATE_runnable )
            vcpu_runstate_change(v, RUNSTATE_offline, NOW());

        SCHED_OP(VCPU2OP(v), sleep, v);
    }

    vcpu_schedule_unlock_irqrestore(lock, flags, v);

    TRACE_2D(TRC_SCHED_SLEEP, v->domain->domain_id, v->vcpu_id);
}

void vcpu_sleep_sync(struct vcpu *v)
{
    vcpu_sleep_nosync(v);

    while ( !vcpu_runnable(v) && v->is_running )
        cpu_relax();

    sync_vcpu_execstate(v);
}

void vcpu_wake(struct vcpu *v)
{
    unsigned long flags;
    spinlock_t *lock = vcpu_schedule_lock_irqsave(v, &flags);

    if ( likely(vcpu_runnable(v)) )
    {
        if ( v->runstate.state >= RUNSTATE_blocked )
            vcpu_runstate_change(v, RUNSTATE_runnable, NOW());
        SCHED_OP(VCPU2OP(v), wake, v);
    }
    else if ( !test_bit(_VPF_blocked, &v->pause_flags) )
    {
        if ( v->runstate.state == RUNSTATE_blocked )
            vcpu_runstate_change(v, RUNSTATE_offline, NOW());
    }

    vcpu_schedule_unlock_irqrestore(lock, flags, v);

    TRACE_2D(TRC_SCHED_WAKE, v->domain->domain_id, v->vcpu_id);
}

void vcpu_unblock(struct vcpu *v)
{
    if ( !test_and_clear_bit(_VPF_blocked, &v->pause_flags) )
        return;

    /* Polling period ends when a VCPU is unblocked. */
    if ( unlikely(v->poll_evtchn != 0) )
    {
        v->poll_evtchn = 0;
        /*
         * We *must* re-clear _VPF_blocked to avoid racing other wakeups of
         * this VCPU (and it then going back to sleep on poll_mask).
         * Test-and-clear is idiomatic and ensures clear_bit not reordered.
         */
        if ( test_and_clear_bit(v->vcpu_id, v->domain->poll_mask) )
            clear_bit(_VPF_blocked, &v->pause_flags);
    }

    vcpu_wake(v);
}

static void vcpu_migrate(struct vcpu *v)
{
    unsigned long flags;
    unsigned int old_cpu, new_cpu;
    spinlock_t *old_lock, *new_lock;
    bool_t pick_called = 0;

    old_cpu = new_cpu = v->processor;
    for ( ; ; )
    {
        /*
         * If per-cpu locks for old and new cpu are different, take the one
         * with the lower lock address first. This avoids dead- or live-locks
         * when this code is running on both cpus at the same time.
         * We need another iteration if the pre-calculated lock addresses
         * are not correct any longer after evaluating old and new cpu holding
         * the locks.
         */

        old_lock = per_cpu(schedule_data, old_cpu).schedule_lock;
        new_lock = per_cpu(schedule_data, new_cpu).schedule_lock;

        if ( old_lock == new_lock )
        {
            spin_lock_irqsave(old_lock, flags);
        }
        else if ( old_lock < new_lock )
        {
            spin_lock_irqsave(old_lock, flags);
            spin_lock(new_lock);
        }
        else
        {
            spin_lock_irqsave(new_lock, flags);
            spin_lock(old_lock);
        }

        old_cpu = v->processor;
        if ( old_lock == per_cpu(schedule_data, old_cpu).schedule_lock )
        {
            /*
             * If we selected a CPU on the previosu iteration, check if it
             * remains suitable for running this vCPU.
             */
            if ( pick_called &&
                 (new_lock == per_cpu(schedule_data, new_cpu).schedule_lock) &&
                 cpumask_test_cpu(new_cpu, v->cpu_affinity) &&
                 cpumask_test_cpu(new_cpu, v->domain->cpupool->cpu_valid) )
                break;

            /* Select a new CPU. */
            new_cpu = SCHED_OP(VCPU2OP(v), pick_cpu, v);
            if ( (new_lock == per_cpu(schedule_data, new_cpu).schedule_lock) &&
                 cpumask_test_cpu(new_cpu, v->domain->cpupool->cpu_valid) )
                break;
            pick_called = 1;
        }
        else
        {
            /*
             * We do not hold the scheduler lock appropriate for this vCPU.
             * Thus we cannot select a new CPU on this iteration. Try again.
             */
            pick_called = 0;
        }

        if ( old_lock != new_lock )
            spin_unlock(new_lock);
        spin_unlock_irqrestore(old_lock, flags);
    }

    /*
     * NB. Check of v->running happens /after/ setting migration flag
     * because they both happen in (different) spinlock regions, and those
     * regions are strictly serialised.
     */
    if ( v->is_running ||
         !test_and_clear_bit(_VPF_migrating, &v->pause_flags) )
    {
        if ( old_lock != new_lock )
            spin_unlock(new_lock);
        spin_unlock_irqrestore(old_lock, flags);
        return;
    }

    /*
     * Transfer urgency status to new CPU before switching CPUs, as once
     * the switch occurs, v->is_urgent is no longer protected by the per-CPU
     * scheduler lock we are holding.
     */
    if ( unlikely(v->is_urgent) && (old_cpu != new_cpu) )
    {
        atomic_inc(&per_cpu(schedule_data, new_cpu).urgent_count);
        atomic_dec(&per_cpu(schedule_data, old_cpu).urgent_count);
    }

    /*
     * Switch to new CPU, then unlock new and old CPU.  This is safe because
     * the lock pointer cant' change while the current lock is held.
     */
    if ( VCPU2OP(v)->migrate )
        SCHED_OP(VCPU2OP(v), migrate, v, new_cpu);
    else
        v->processor = new_cpu;


    if ( old_lock != new_lock )
        spin_unlock(new_lock);
    spin_unlock_irqrestore(old_lock, flags);

    if ( old_cpu != new_cpu )
        evtchn_move_pirqs(v);

    /* Wake on new CPU. */
    vcpu_wake(v);
}

/*
 * Force a VCPU through a deschedule/reschedule path.
 * For example, using this when setting the periodic timer period means that
 * most periodic-timer state need only be touched from within the scheduler
 * which can thus be done without need for synchronisation.
 */
void vcpu_force_reschedule(struct vcpu *v)
{
    spinlock_t *lock = vcpu_schedule_lock_irq(v);

    if ( v->is_running )
        set_bit(_VPF_migrating, &v->pause_flags);
    vcpu_schedule_unlock_irq(lock, v);

    if ( test_bit(_VPF_migrating, &v->pause_flags) )
    {
        vcpu_sleep_nosync(v);
        vcpu_migrate(v);
    }
}

void restore_vcpu_affinity(struct domain *d)
{
    struct vcpu *v;

    for_each_vcpu ( d, v )
    {
        spinlock_t *lock = vcpu_schedule_lock_irq(v);

        if ( v->affinity_broken )
        {
            printk(XENLOG_DEBUG "Restoring affinity for d%dv%d\n",
                   d->domain_id, v->vcpu_id);
            cpumask_copy(v->cpu_affinity, v->cpu_affinity_saved);
            v->affinity_broken = 0;
        }

        if ( v->processor == smp_processor_id() )
        {
            set_bit(_VPF_migrating, &v->pause_flags);
            vcpu_schedule_unlock_irq(lock, v);
            vcpu_sleep_nosync(v);
            vcpu_migrate(v);
        }
        else
        {
            vcpu_schedule_unlock_irq(lock, v);
        }
    }

    domain_update_node_affinity(d);
}

/*
 * This function is used by cpu_hotplug code from stop_machine context
 * and from cpupools to switch schedulers on a cpu.
 */
int cpu_disable_scheduler(unsigned int cpu)
{
    struct domain *d;
    struct vcpu *v;
    struct cpupool *c;
    cpumask_t online_affinity;
    int    ret = 0;

    c = per_cpu(cpupool, cpu);
    if ( c == NULL )
        return ret;

    for_each_domain_in_cpupool ( d, c )
    {
        for_each_vcpu ( d, v )
        {
            unsigned long flags;
            spinlock_t *lock = vcpu_schedule_lock_irqsave(v, &flags);

            cpumask_and(&online_affinity, v->cpu_affinity, c->cpu_valid);
            if ( cpumask_empty(&online_affinity) &&
                 cpumask_test_cpu(cpu, v->cpu_affinity) )
            {
                printk(XENLOG_DEBUG "Breaking affinity for d%dv%d\n",
                        d->domain_id, v->vcpu_id);

                if (system_state == SYS_STATE_suspend)
                {
                    cpumask_copy(v->cpu_affinity_saved, v->cpu_affinity);
                    v->affinity_broken = 1;
                }

                cpumask_setall(v->cpu_affinity);
            }

            if ( v->processor == cpu )
            {
                set_bit(_VPF_migrating, &v->pause_flags);
                vcpu_schedule_unlock_irqrestore(lock, flags, v);
                vcpu_sleep_nosync(v);
                vcpu_migrate(v);
            }
            else
                vcpu_schedule_unlock_irqrestore(lock, flags, v);

            /*
             * A vcpu active in the hypervisor will not be migratable.
             * The caller should try again after releasing and reaquiring
             * all locks.
             */
            if ( v->processor == cpu )
                ret = -EAGAIN;
        }

        domain_update_node_affinity(d);
    }

    return ret;
}

void sched_set_node_affinity(struct domain *d, nodemask_t *mask)
{
    SCHED_OP(DOM2OP(d), set_node_affinity, d, mask);
}

int vcpu_set_affinity(struct vcpu *v, const cpumask_t *affinity)
{
    cpumask_t online_affinity;
    cpumask_t *online;
    spinlock_t *lock;

    if ( v->domain->is_pinned )
        return -EINVAL;
    online = VCPU2ONLINE(v);
    cpumask_and(&online_affinity, affinity, online);
    if ( cpumask_empty(&online_affinity) )
        return -EINVAL;

    lock = vcpu_schedule_lock_irq(v);

    cpumask_copy(v->cpu_affinity, affinity);

    /* Always ask the scheduler to re-evaluate placement
     * when changing the affinity */
    set_bit(_VPF_migrating, &v->pause_flags);

    vcpu_schedule_unlock_irq(lock, v);

    domain_update_node_affinity(v->domain);

    if ( test_bit(_VPF_migrating, &v->pause_flags) )
    {
        vcpu_sleep_nosync(v);
        vcpu_migrate(v);
    }

    return 0;
}

/* Block the currently-executing domain until a pertinent event occurs. */
void vcpu_block(void)
{
    struct vcpu *v = current;

    set_bit(_VPF_blocked, &v->pause_flags);

    /* Check for events /after/ blocking: avoids wakeup waiting race. */
    if ( local_events_need_delivery() )
    {
        clear_bit(_VPF_blocked, &v->pause_flags);
    }
    else
    {
        TRACE_2D(TRC_SCHED_BLOCK, v->domain->domain_id, v->vcpu_id);
        raise_softirq(SCHEDULE_SOFTIRQ);
    }
}

static void vcpu_block_enable_events(void)
{
    local_event_delivery_enable();
    vcpu_block();
}

static long do_poll(struct sched_poll *sched_poll)
{
    struct vcpu   *v = current;
    struct domain *d = v->domain;
    evtchn_port_t  port;
    long           rc;
    unsigned int   i;

    /* Fairly arbitrary limit. */
    if ( sched_poll->nr_ports > 128 )
        return -EINVAL;

    if ( !guest_handle_okay(sched_poll->ports, sched_poll->nr_ports) )
        return -EFAULT;

    set_bit(_VPF_blocked, &v->pause_flags);
    v->poll_evtchn = -1;
    set_bit(v->vcpu_id, d->poll_mask);

#ifndef CONFIG_X86 /* set_bit() implies mb() on x86 */
    /* Check for events /after/ setting flags: avoids wakeup waiting race. */
    smp_mb();

    /*
     * Someone may have seen we are blocked but not that we are polling, or
     * vice versa. We are certainly being woken, so clean up and bail. Beyond
     * this point others can be guaranteed to clean up for us if they wake us.
     */
    rc = 0;
    if ( (v->poll_evtchn == 0) ||
         !test_bit(_VPF_blocked, &v->pause_flags) ||
         !test_bit(v->vcpu_id, d->poll_mask) )
        goto out;
#endif

    rc = 0;
    if ( local_events_need_delivery() )
        goto out;

    for ( i = 0; i < sched_poll->nr_ports; i++ )
    {
        rc = -EFAULT;
        if ( __copy_from_guest_offset(&port, sched_poll->ports, i, 1) )
            goto out;

        rc = -EINVAL;
        if ( port >= d->max_evtchns )
            goto out;

        rc = 0;
        if ( evtchn_port_is_pending(d, evtchn_from_port(d, port)) )
            goto out;
    }

    if ( sched_poll->nr_ports == 1 )
        v->poll_evtchn = port;

    if ( sched_poll->timeout != 0 )
        set_timer(&v->poll_timer, sched_poll->timeout);

    TRACE_2D(TRC_SCHED_BLOCK, d->domain_id, v->vcpu_id);
    raise_softirq(SCHEDULE_SOFTIRQ);

    return 0;

 out:
    v->poll_evtchn = 0;
    clear_bit(v->vcpu_id, d->poll_mask);
    clear_bit(_VPF_blocked, &v->pause_flags);
    return rc;
}

/* Voluntarily yield the processor for this allocation. */
static long do_yield(void)
{
    struct vcpu * v=current;
    spinlock_t *lock = vcpu_schedule_lock_irq(v);

    SCHED_OP(VCPU2OP(v), yield, v);
    vcpu_schedule_unlock_irq(lock, v);

    TRACE_2D(TRC_SCHED_YIELD, current->domain->domain_id, current->vcpu_id);
    raise_softirq(SCHEDULE_SOFTIRQ);
    return 0;
}

static void domain_watchdog_timeout(void *data)
{
    struct domain *d = data;

    if ( d->is_shutting_down || d->is_dying )
        return;

    printk("Watchdog timer fired for domain %u\n", d->domain_id);
    domain_shutdown(d, SHUTDOWN_watchdog);
}

static long domain_watchdog(struct domain *d, uint32_t id, uint32_t timeout)
{
    if ( id > NR_DOMAIN_WATCHDOG_TIMERS )
        return -EINVAL;

    spin_lock(&d->watchdog_lock);

    if ( id == 0 )
    {
        for ( id = 0; id < NR_DOMAIN_WATCHDOG_TIMERS; id++ )
        {
            if ( test_and_set_bit(id, &d->watchdog_inuse_map) )
                continue;
            set_timer(&d->watchdog_timer[id], NOW() + SECONDS(timeout));
            break;
        }
        spin_unlock(&d->watchdog_lock);
        return id == NR_DOMAIN_WATCHDOG_TIMERS ? -ENOSPC : id + 1;
    }

    id -= 1;
    if ( !test_bit(id, &d->watchdog_inuse_map) )
    {
        spin_unlock(&d->watchdog_lock);
        return -EINVAL;
    }

    if ( timeout == 0 )
    {
        stop_timer(&d->watchdog_timer[id]);
        clear_bit(id, &d->watchdog_inuse_map);
    }
    else
    {
        set_timer(&d->watchdog_timer[id], NOW() + SECONDS(timeout));
    }

    spin_unlock(&d->watchdog_lock);
    return 0;
}

void watchdog_domain_init(struct domain *d)
{
    unsigned int i;

    spin_lock_init(&d->watchdog_lock);

    d->watchdog_inuse_map = 0;

    for ( i = 0; i < NR_DOMAIN_WATCHDOG_TIMERS; i++ )
        init_timer(&d->watchdog_timer[i], domain_watchdog_timeout, d, 0);
}

void watchdog_domain_destroy(struct domain *d)
{
    unsigned int i;

    for ( i = 0; i < NR_DOMAIN_WATCHDOG_TIMERS; i++ )
        kill_timer(&d->watchdog_timer[i]);
}

long do_sched_op_compat(int cmd, unsigned long arg)
{
    long ret = 0;

    switch ( cmd )
    {
    case SCHEDOP_yield:
    {
        ret = do_yield();
        break;
    }

    case SCHEDOP_block:
    {
        vcpu_block_enable_events();
        break;
    }

    case SCHEDOP_shutdown:
    {
        TRACE_3D(TRC_SCHED_SHUTDOWN,
                 current->domain->domain_id, current->vcpu_id, arg);
        domain_shutdown(current->domain, (u8)arg);
        break;
    }

    default:
        ret = -ENOSYS;
    }

    return ret;
}

typedef long ret_t;

#endif /* !COMPAT */

ret_t do_sched_op(int cmd, XEN_GUEST_HANDLE_PARAM(void) arg)
{
    ret_t ret = 0;

    switch ( cmd )
    {
    case SCHEDOP_yield:
    {
        ret = do_yield();
        break;
    }

    case SCHEDOP_block:
    {
        vcpu_block_enable_events();
        break;
    }

    case SCHEDOP_shutdown:
    {
        struct sched_shutdown sched_shutdown;

        ret = -EFAULT;
        if ( copy_from_guest(&sched_shutdown, arg, 1) )
            break;

        ret = 0;
        TRACE_3D(TRC_SCHED_SHUTDOWN,
                 current->domain->domain_id, current->vcpu_id,
                 sched_shutdown.reason);
        domain_shutdown(current->domain, (u8)sched_shutdown.reason);

        break;
    }

    case SCHEDOP_shutdown_code:
    {
        struct sched_shutdown sched_shutdown;
        struct domain *d = current->domain;

        ret = -EFAULT;
        if ( copy_from_guest(&sched_shutdown, arg, 1) )
            break;

        TRACE_3D(TRC_SCHED_SHUTDOWN_CODE,
                 d->domain_id, current->vcpu_id, sched_shutdown.reason);

        spin_lock(&d->shutdown_lock);
        if ( d->shutdown_code == -1 )
            d->shutdown_code = (u8)sched_shutdown.reason;
        spin_unlock(&d->shutdown_lock);

        ret = 0;
        break;
    }

    case SCHEDOP_poll:
    {
        struct sched_poll sched_poll;

        ret = -EFAULT;
        if ( copy_from_guest(&sched_poll, arg, 1) )
            break;

        ret = do_poll(&sched_poll);

        break;
    }

    case SCHEDOP_remote_shutdown:
    {
        struct domain *d;
        struct sched_remote_shutdown sched_remote_shutdown;

        ret = -EFAULT;
        if ( copy_from_guest(&sched_remote_shutdown, arg, 1) )
            break;

        ret = -ESRCH;
        d = rcu_lock_domain_by_id(sched_remote_shutdown.domain_id);
        if ( d == NULL )
            break;

        ret = xsm_schedop_shutdown(XSM_DM_PRIV, current->domain, d);
        if ( ret )
        {
            rcu_unlock_domain(d);
            return ret;
        }

        domain_shutdown(d, (u8)sched_remote_shutdown.reason);

        rcu_unlock_domain(d);
        ret = 0;

        break;
    }

    case SCHEDOP_watchdog:
    {
        struct sched_watchdog sched_watchdog;

        ret = -EFAULT;
        if ( copy_from_guest(&sched_watchdog, arg, 1) )
            break;

        ret = domain_watchdog(
            current->domain, sched_watchdog.id, sched_watchdog.timeout);
        break;
    }

    default:
        ret = -ENOSYS;
    }

    return ret;
}

#ifndef COMPAT

/* Per-vcpu oneshot-timer hypercall. */
long do_set_timer_op(s_time_t timeout)
{
    struct vcpu *v = current;
    s_time_t offset = timeout - NOW();

    if ( timeout == 0 )
    {
        stop_timer(&v->singleshot_timer);
    }
    else if ( unlikely(timeout < 0) || /* overflow into 64th bit? */
              unlikely((offset > 0) && ((uint32_t)(offset >> 50) != 0)) )
    {
        /*
         * Linux workaround: occasionally we will see timeouts a long way in 
         * the future due to wrapping in Linux's jiffy time handling. We check 
         * for timeouts wrapped negative, and for positive timeouts more than 
         * about 13 days in the future (2^50ns). The correct fix is to trigger 
         * an interrupt immediately (since Linux in fact has pending work to 
         * do in this situation). However, older guests also set a long timeout
         * when they have *no* pending timers at all: setting an immediate
         * timeout in this case can burn a lot of CPU. We therefore go for a
         * reasonable middleground of triggering a timer event in 100ms.
         */
        gdprintk(XENLOG_INFO,
                 "Warning: huge timeout set by vcpu %d: %"PRIx64"\n",
                 v->vcpu_id, (uint64_t)timeout);
        set_timer(&v->singleshot_timer, NOW() + MILLISECS(100));
    }
    else
    {
        migrate_timer(&v->singleshot_timer, smp_processor_id());
        set_timer(&v->singleshot_timer, timeout);
    }

    return 0;
}

/* sched_id - fetch ID of current scheduler */
int sched_id(void)
{
    return ops.sched_id;
}

/* Adjust scheduling parameter for a given domain. */
long sched_adjust(struct domain *d, struct xen_domctl_scheduler_op *op)
{
    long ret;

    ret = xsm_domctl_scheduler_op(XSM_HOOK, d, op->cmd);
    if ( ret )
        return ret;

    if ( (op->sched_id != DOM2OP(d)->sched_id) ||
         ((op->cmd != XEN_DOMCTL_SCHEDOP_putinfo) &&
          (op->cmd != XEN_DOMCTL_SCHEDOP_getinfo)) )
        return -EINVAL;

    /* NB: the pluggable scheduler code needs to take care
     * of locking by itself. */
    if ( (ret = SCHED_OP(DOM2OP(d), adjust, d, op)) == 0 )
        TRACE_1D(TRC_SCHED_ADJDOM, d->domain_id);

    return ret;
}

long sched_adjust_global(struct xen_sysctl_scheduler_op *op)
{
    struct cpupool *pool;
    int rc;

    rc = xsm_sysctl_scheduler_op(XSM_HOOK, op->cmd);
    if ( rc )
        return rc;

    if ( (op->cmd != XEN_DOMCTL_SCHEDOP_putinfo) &&
         (op->cmd != XEN_DOMCTL_SCHEDOP_getinfo) )
        return -EINVAL;

    pool = cpupool_get_by_id(op->cpupool_id);
    if ( pool == NULL )
        return -ESRCH;

    rc = ((op->sched_id == pool->sched->sched_id)
          ? SCHED_OP(pool->sched, adjust_global, op) : -EINVAL);

    cpupool_put(pool);

    return rc;
}

static void vcpu_periodic_timer_work(struct vcpu *v)
{
    s_time_t now = NOW();
    s_time_t periodic_next_event;

    if ( v->periodic_period == 0 )
        return;

    periodic_next_event = v->periodic_last_event + v->periodic_period;

    if ( now >= periodic_next_event )
    {
        send_timer_event(v);
        v->periodic_last_event = now;
        periodic_next_event = now + v->periodic_period;
    }

    migrate_timer(&v->periodic_timer, smp_processor_id());
    set_timer(&v->periodic_timer, periodic_next_event);
}

/* 
 * The main function
 * - deschedule the current domain (scheduler independent).
 * - pick a new domain (scheduler dependent).
 */
static void schedule(void)
{
    struct vcpu          *prev = current, *next = NULL;
    s_time_t              now = NOW();
    struct scheduler     *sched;
    unsigned long        *tasklet_work = &this_cpu(tasklet_work_to_do);
    bool_t                tasklet_work_scheduled = 0;
    struct schedule_data *sd;
    spinlock_t           *lock;
    struct task_slice     next_slice;
    int cpu = smp_processor_id();

    unsigned long long ia32_pmc0, ia32_pmc1, ia32_pmc2, ia32_pmc3, 
                    ia32_fixed_ctr0, ia32_fixed_ctr1, ia32_fixed_ctr2, 
                    msr_rapl_tmp, energy_units, msr_pkg_energy_status, 
                    msr_pp0_energy_status,mperf_reg,aperf_reg;

    /* TODO - Enable these if you want to log ia32_pmc4 to ia32_pmc8 */
    /* unsigned long long msr_pp1_energy_status, msr_dram_energy_status; */

    /* TODO - Enable these if you want to log ia32_pmc4 to ia32_pmc8 */
    /* Please read Vol. 3B 18-41 and check SMT */
    //unsigned long long ia32_pmc4, ia32_pmc5, ia32_pmc6, ia32_pmc7;

    ASSERT_NOT_IN_ATOMIC();

    SCHED_STAT_CRANK(sched_run);

    sd = &this_cpu(schedule_data);

    /* Update tasklet scheduling status. */
    switch ( *tasklet_work )
    {
    case TASKLET_enqueued:
        set_bit(_TASKLET_scheduled, tasklet_work);
    case TASKLET_enqueued|TASKLET_scheduled:
        tasklet_work_scheduled = 1;
        break;
    case TASKLET_scheduled:
        clear_bit(_TASKLET_scheduled, tasklet_work);
    case 0:
        /*tasklet_work_scheduled = 0;*/
        break;
    default:
        BUG();
    }

    lock = pcpu_schedule_lock_irq(cpu);

    stop_timer(&sd->s_timer);
    
    /* get policy-specific decision on scheduling... */
    sched = this_cpu(scheduler);




    /* XEMPOWER_TAG */
    /*
     * Notes on registers:
     *  ecx = MSR identifier:        unsigned int ecx;
     *  eax = low double word:       unsigned int eax;
     *  edx = high double word:      unsigned int edx;
     *  return value = quad word:    unsigned long long value;
     * 
     * Usage:
     *  write_msr(ecx, eax, edx);
     */

     /*PMC ADDRESSES*/
     #define IA32_PERF_GLOBAL_CTRL      0x38f
     #define IA32_PERF_FIXED_CTR_CTRL   0x38d
     #define IA32_FIXED_CTR0            0x309
     #define IA32_FIXED_CTR1            0x30a
     #define IA32_FIXED_CTR2            0x30b
     #define IA32_PMC0                  0xc1
     #define IA32_PMC1                  0xc2
     #define IA32_PMC2                  0xc3
     #define IA32_PMC3                  0xc4
     #define IA32_PMC4                  0xc5
     #define IA32_PMC5                  0xc6
     #define IA32_PMC6                  0xc7
     #define IA32_PMC7                  0xc8
     #define IA32_MPERF_REG             0xe7
     #define IA32_APERF_REG             0xe8
     #define IA32_PERFEVTSEL0           0x186
     #define IA32_PERFEVTSEL1           0x187
     #define IA32_PERFEVTSEL2           0x188 
     #define IA32_PERFEVTSEL3           0x189 
     #define IA32_PERFEVTSEL4           0x18a
     #define IA32_PERFEVTSEL5           0x18b
     #define IA32_PERFEVTSEL6           0x18c
     #define IA32_PERFEVTSEL7           0x18d

     /*RAPL COUNTERS ADDRESSES*/
     #define MSR_RAPL_POWER_UNIT        0x606
     #define MSR_PKG_ENERGY_STATUS      0x611
     #define MSR_PP0_ENERGY_STATUS      0x639
     #define MSR_PP1_ENERGY_STATUS      0x641
     #define MSR_DRAM_ENERGY_STATUS     0x619


     /*EVENTS SELECT CODE*/
     #define LLC_REF                    0x2e
     #define LLC_MISSES                 0x2e
     #define UN_CORE_CYCLE              0x3c
     #define INST_RET                   0xc0
     #define UOPS_RET                   0xc2
     #define BR_INSTR_RET               0xc4
     #define BR_MISS_RET                0xc5
     #define MEM_UOPS_RETIRED           0xd0
     #define MEM_LOAD_UOPS_RET          0xd1
     #define MEM_LOAD_UOPS_LLC_HIT_RET  0xd2

     /*UMASK EVENT SELECT*/
     #define UMASK_00                   0x0000
     #define UMASK_01                   0x0100
     #define UMASK_02                   0x0200
     #define UMASK_04                   0x0400
     #define UMASK_08                   0x0800
     #define UMASK_10                   0x1000
     #define UMASK_11                   0x1100
     #define UMASK_12                   0x1200
     #define UMASK_20                   0x2000
     #define UMASK_21                   0x2100
     #define UMASK_40                   0x4000 
     #define UMASK_41                   0x4100
     #define UMASK_42                   0x4200
     #define UMASK_4F                   0x4f00
     #define UMASK_81                   0x8100
     #define UMASK_82                   0x8200


     /*PEBS MASK*/
     #define PEBS_ENABLE                0x400000
     #define PEBS_USER_MODE             0x10000
     #define PEBS_OS_MODE               0x20000

     /*PREDEFINED PEBS VOL. 3B 18-9*/
     #define UN_CORE_CYCLE_USER_MODE                            (PEBS_ENABLE | PEBS_USER_MODE | UN_CORE_CYCLE)
     #define INST_RET_USER_MODE                                 (PEBS_ENABLE | PEBS_USER_MODE | INST_RET)
     #define UN_CORE_REF_CYCLE_USER_MODE                        (PEBS_ENABLE | PEBS_USER_MODE | UMASK_01 | UN_CORE_CYCLE)
     #define LLC_REF_USER_MODE                                  (PEBS_ENABLE | PEBS_USER_MODE | UMASK_4F | LLC_REF)
     #define LLC_MISSES_USER_MODE                               (PEBS_ENABLE | PEBS_USER_MODE | UMASK_41 | LLC_REF)
     #define BR_INSTR_RET_USER_MODE                             (PEBS_ENABLE | PEBS_USER_MODE | BR_INSTR_RET)
     #define BR_MISS_RET_USER_MODE                              (PEBS_ENABLE | PEBS_USER_MODE | BR_MISS_RET)

     /*GENERIC PEBS SANDY BRIDGE VOL. 3B 18-47*/
     /*ONLY on IA32_PMC1*/
     #define INST_RET_USER_MODE_PREC_DIST                       (PEBS_ENABLE | PEBS_USER_MODE | UMASK_01 | INST_RET)
     /*ON EACH IA32_PMCx*/
     #define UOPS_RET_USER_MODE_ALL                             (PEBS_ENABLE | PEBS_USER_MODE | UMASK_01 | UOPS_RET)
     #define UOPS_RET_USER_MODE_RETIRED_SLOTS                   (PEBS_ENABLE | PEBS_USER_MODE | UMASK_02 | UOPS_RET)

     #define BR_INSTR_RET_USER_MODE_CONDITIONAL                 (PEBS_ENABLE | PEBS_USER_MODE | UMASK_01 | BR_INSTR_RET)
     #define BR_INSTR_RET_USER_MODE_NEAR_CALL                   (PEBS_ENABLE | PEBS_USER_MODE | UMASK_02 | BR_INSTR_RET)
     #define BR_INSTR_RET_USER_MODE_ALL_BRANCHES                (PEBS_ENABLE | PEBS_USER_MODE | UMASK_04 | BR_INSTR_RET)
     #define BR_INSTR_RET_USER_MODE_NEAR_RETURN                 (PEBS_ENABLE | PEBS_USER_MODE | UMASK_08 | BR_INSTR_RET)
     #define BR_INSTR_RET_USER_MODE_NEAR_TAKEN                  (PEBS_ENABLE | PEBS_USER_MODE | UMASK_20 | BR_INSTR_RET)

     #define BR_MISP_RET_USER_MODE_CONDITIONAL                  (PEBS_ENABLE | PEBS_USER_MODE | UMASK_01 | BR_MISS_RET)
     #define BR_MISP_RET_USER_MODE_NEAR_CALL                    (PEBS_ENABLE | PEBS_USER_MODE | UMASK_02 | BR_MISS_RET)
     #define BR_MISP_RET_USER_MODE_ALL_BRANCHES                 (PEBS_ENABLE | PEBS_USER_MODE | UMASK_04 | BR_MISS_RET)
     #define BR_MISP_RET_USER_MODE_NOT_TAKEN                    (PEBS_ENABLE | PEBS_USER_MODE | UMASK_10 | BR_MISS_RET)
     #define BR_MISP_RET_USER_MODE_TAKEN                        (PEBS_ENABLE | PEBS_USER_MODE | UMASK_20 | BR_MISS_RET)

     #define MEM_UOPS_RET_USER_MODE_STLB_MISS_LOADS             (PEBS_ENABLE | PEBS_USER_MODE | UMASK_11 | MEM_UOPS_RETIRED)
     #define MEM_UOPS_RET_USER_MODE_STLB_MISS_STORE             (PEBS_ENABLE | PEBS_USER_MODE | UMASK_12 | MEM_UOPS_RETIRED)
     #define MEM_UOPS_RET_USER_MODE_LOCK_LOADS                  (PEBS_ENABLE | PEBS_USER_MODE | UMASK_21 | MEM_UOPS_RETIRED)
     #define MEM_UOPS_RET_USER_MODE_SPLIT_LOADS                 (PEBS_ENABLE | PEBS_USER_MODE | UMASK_41 | MEM_UOPS_RETIRED)
     #define MEM_UOPS_RET_USER_MODE_SPLIT_STORES                (PEBS_ENABLE | PEBS_USER_MODE | UMASK_42 | MEM_UOPS_RETIRED)
     #define MEM_UOPS_RET_USER_MODE_ALL_LOADS                   (PEBS_ENABLE | PEBS_USER_MODE | UMASK_81 | MEM_UOPS_RETIRED)
     #define MEM_UOPS_RET_USER_MODE_ALL_STORES                  (PEBS_ENABLE | PEBS_USER_MODE | UMASK_82 | MEM_UOPS_RETIRED)

     #define MEM_LOAD_UOPS_RET_USER_MODE_L1_HIT                 (PEBS_ENABLE | PEBS_USER_MODE | UMASK_01 | MEM_LOAD_UOPS_RET)
     #define MEM_LOAD_UOPS_RET_USER_MODE_L2_HIT                 (PEBS_ENABLE | PEBS_USER_MODE | UMASK_02 | MEM_LOAD_UOPS_RET)
     #define MEM_LOAD_UOPS_RET_USER_MODE_L3_HIT                 (PEBS_ENABLE | PEBS_USER_MODE | UMASK_04 | MEM_LOAD_UOPS_RET)
     #define MEM_LOAD_UOPS_RET_USER_MODE_HIT_LFB                (PEBS_ENABLE | PEBS_USER_MODE | UMASK_40 | MEM_LOAD_UOPS_RET)

     #define MEM_LOAD_UOPS_LLC_HIT_RET_USER_MODE_XSNP_MISS      (PEBS_ENABLE | PEBS_USER_MODE | UMASK_01 | MEM_LOAD_UOPS_LLC_HIT_RET)
     #define MEM_LOAD_UOPS_LLC_HIT_RET_USER_MODE_XSNP_HIT       (PEBS_ENABLE | PEBS_USER_MODE | UMASK_02 | MEM_LOAD_UOPS_LLC_HIT_RET)
     #define MEM_LOAD_UOPS_LLC_HIT_RET_USER_MODE_XSNP_HITM      (PEBS_ENABLE | PEBS_USER_MODE | UMASK_04 | MEM_LOAD_UOPS_LLC_HIT_RET)
     #define MEM_LOAD_UOPS_LLC_HIT_RET_USER_MODE_XSNP_NONE      (PEBS_ENABLE | PEBS_USER_MODE | UMASK_08 | MEM_LOAD_UOPS_LLC_HIT_RET)


     /*GENERAL VALUES*/
     #define ZERO_VALUE                 0x00
     #define ENERGY_UNIT_MASK           0x1f
     #define ENERGY_UNIT_SHIFT          8
     #define ENABLE_PMC_0_3             0x0f
     #define ENABLE_PMC_4_7             0xf0
 //  #define ENABLE_PMC_0_7             ENABLE_PMC_4_7 | ENABLE_PMC_0_3
     #define ENABLE_CTR_0_2             0x07   
     #define ENABLE_CTR_0_2_USER_MODE   0x222 

    /* Read counters */
    write_msr(IA32_PERF_GLOBAL_CTRL, ZERO_VALUE, ZERO_VALUE);           /* ia32_perf_global_ctrl: disable 4 PMCs & 3 FFCs */
    write_msr(IA32_PERF_FIXED_CTR_CTRL, ZERO_VALUE, ZERO_VALUE);        /* ia32_perf_fixed_ctr_ctrl: clean up FFC ctrls */
    ia32_pmc0 = read_msr(IA32_PMC0);                                    /* ia32_pmc0: read value (Vol. 3C, Page 35-5) */ 
    ia32_pmc1 = read_msr(IA32_PMC1);                                    /* ia32_pmc1: read value (Vol. 3C, Page 35-5) */    
    ia32_pmc2 = read_msr(IA32_PMC2);                                    /* ia32_pmc2: read value (Vol. 3C, Page 35-5) */    
    ia32_pmc3 = read_msr(IA32_PMC3);                                    /* ia32_pmc3: read value (Vol. 3C, Page 35-5) */    
//  ia32_pmc4 = read_msr(IA32_PMC4);                                    /* ia32_pmc4: read value (Vol. 3C, Page 35-5) */ 
//  ia32_pmc5 = read_msr(IA32_PMC5);                                    /* ia32_pmc5: read value (Vol. 3C, Page 35-5) */    
//  ia32_pmc6 = read_msr(IA32_PMC6);                                    /* ia32_pmc6: read value (Vol. 3C, Page 35-5) */    
//  ia32_pmc7 = read_msr(IA32_PMC7);                                    /* ia32_pmc7: read value (Vol. 3C, Page 35-5) */    
    ia32_fixed_ctr0 = read_msr(IA32_FIXED_CTR0);                        /* ia32_fixed_ctr0: read value (Vol. 3C, Page 35-18) - Fixed to: Instr_Retired.Any */
    ia32_fixed_ctr1 = read_msr(IA32_FIXED_CTR1);                        /* ia32_fixed_ctr1: read value (Vol. 3C, Page 35-18) - Fixed to: CPU_CLK_Unhalted.Core */
    ia32_fixed_ctr2 = read_msr(IA32_FIXED_CTR2);                        /* ia32_fixed_ctr2: read value (Vol. 3C, Page 35-18) - Fixed to: CPU_CLK_Unhalted.Ref */

    CUSTOM_LOGT("Entering: schedule() ------------------------\n");
    CUSTOM_LOGT("Instr_Retired.Any:         %7lld\n", ia32_fixed_ctr0);
    CUSTOM_LOGT("CPU_CLK_Unhalted.Core:     %7lld\n", ia32_fixed_ctr1);
    CUSTOM_LOGT("CPU_CLK_Unhalted.Ref:      %7lld\n", ia32_fixed_ctr2);
    CUSTOM_LOGT("LLC Reference:             %7lld\n", ia32_pmc0);
    CUSTOM_LOGT("LLC Misses:                %7lld\n", ia32_pmc1);
    CUSTOM_LOGT("Branch Instr. Retired:     %7lld\n", ia32_pmc2);
    CUSTOM_LOGT("Branch Misses Retired:     %7lld\n", ia32_pmc3);
/*  CUSTOM_LOGT("PMC4:     %7lld\n", ia32_pmc4);
    CUSTOM_LOGT("PMC5:     %7lld\n", ia32_pmc5);
    CUSTOM_LOGT("PMC6:     %7lld\n", ia32_pmc6);
    CUSTOM_LOGT("PMC7:     %7lld\n", ia32_pmc7);*/

    /*APERF AND MPERF registers Vol 3b 14-2*/
    /*Used to compute current frequency*/
    mperf_reg = read_msr(IA32_MPERF_REG);
    aperf_reg = read_msr(IA32_APERF_REG);

    CUSTOM_LOGT("MPerf register:            %7lld\n",mperf_reg);
    CUSTOM_LOGT("APerf register:            %7lld\n",aperf_reg);

    /* RAPL usage references: Vol. 3B 14-33 */
    /* Calculate the units used */
    msr_rapl_tmp = read_msr(MSR_RAPL_POWER_UNIT);                                                   /* MSR_RAPL_POWER_UNIT (Vol. 3C 35-187)*/
    energy_units = (unsigned long long)((msr_rapl_tmp>>ENERGY_UNIT_SHIFT)&ENERGY_UNIT_MASK);        // pow(0.5,(unsigned long long)((msr_rapl_tmp>>8)&0x1f));
    msr_pkg_energy_status = read_msr(MSR_PKG_ENERGY_STATUS);                                        /* MSR_PKG_ENERGY_STATUS */
    msr_pp0_energy_status = read_msr(MSR_PP0_ENERGY_STATUS);                                        /* MSR_PP0_ENERGY_STATUS */
    // msr_pp1_energy_status = read_msr(MSR_PP1_ENERGY_STATUS);                                     /* MSR_PP1_ENERGY_STATUS */
    // msr_dram_energy_status = read_msr(MSR_DRAM_ENERGY_STATUS);                                   /* MSR_DRAM_ENERGY_STATUS */

    CUSTOM_LOGT("Energy units:    1/2^%lld J\n", energy_units);                                     // printk("Energy units = %.8fJ\n",energy_units);
    CUSTOM_LOGT("Package energy:  %lld * 1/2^%lld J\n", msr_pkg_energy_status,energy_units);        // printk("Package energy: %.6fJ\n",msr_pkg_energy_status);    
    CUSTOM_LOGT("PP0 energy:      %lld * 1/2^%lld J\n", msr_pp0_energy_status,energy_units);        // printk("PP0 energy: %.6fJ\n",msr_pp0_energy_status);    
    // CUSTOM_LOGT("PP1 energy:      %lld * 1/2^%lld J\n", msr_pp1_energy_status,energy_units);     // printk("PP1 energy: %.6fJ\n",msr_pp1_energy_status);    
    // CUSTOM_LOGT("DRAM energy:     %lld * 1/2^%lld J\n", msr_dram_energy_status,energy_units);    // printk("DRAM energy: %.6fJ\n",msr_dram_energy_status);  

    /* Tracing RAPL and PMCs information */
    TRACE_6D(TRC_POWER_RAPL, 
             prev->domain->domain_id, prev->vcpu_id,
             msr_pkg_energy_status, msr_pp0_energy_status,
             0, 0);
    /* TODO - Replace this to enable PP1 and/or DRAM traces */
    //          msr_pp1_energy_status, msr_dram_energy_status);

    TRACE_5D(TRC_POWER_CTR, 
            prev->domain->domain_id, prev->vcpu_id,
            ia32_fixed_ctr0, ia32_fixed_ctr1, ia32_fixed_ctr2);

    TRACE_6D(TRC_POWER_PMC4, 
            prev->domain->domain_id, prev->vcpu_id,
            ia32_pmc0, ia32_pmc1, ia32_pmc2, ia32_pmc3);

    TRACE_4D(TRC_POWER_FREQ,prev->domain->domain_id, prev->vcpu_id,aperf_reg,mperf_reg);

    /* TODO - Add tracing for pmc4-8 */
    /*    TRACE_6D(TRC_POWER_PMC8, 
            prev->domain->domain_id, prev->vcpu_id,
            ia32_pmc4, ia32_pmc5, ia32_pmc6, ia32_pmc7);
    */
    /* Reset counters and start monitoring */
    write_msr(IA32_PERF_GLOBAL_CTRL, ZERO_VALUE, ZERO_VALUE);                   /* writing on: ia32_perf_global_ctrl: disable 8 PMCs (Programmable Counters) & 3 FFCs (Fixed Function Counters, count just one type of event) */
    write_msr(IA32_PMC0, ZERO_VALUE, ZERO_VALUE);                               /* writing on: ia32_pmc0: zero value (Vol. 3C, Page 35-5) */   
    write_msr(IA32_PMC1, ZERO_VALUE, ZERO_VALUE);                               /* writing on: ia32_pmc1: zero value (Vol. 3C, Page 35-5) */
    write_msr(IA32_PMC2, ZERO_VALUE, ZERO_VALUE);                               /* writing on: ia32_pmc2: zero value (Vol. 3C, Page 35-5) */
    write_msr(IA32_PMC3, ZERO_VALUE, ZERO_VALUE);                               /* writing on: ia32_pmc3: zero value (Vol. 3C, Page 35-5) */
//  write_msr(IA32_PMC4, ZERO_VALUE, ZERO_VALUE);                               /* writing on: ia32_pmc4: zero value (Vol. 3C, Page 35-5) */   
//  write_msr(IA32_PMC5, ZERO_VALUE, ZERO_VALUE);                               /* writing on: ia32_pmc5: zero value (Vol. 3C, Page 35-5) */
//  write_msr(IA32_PMC6, ZERO_VALUE, ZERO_VALUE);                               /* writing on: ia32_pmc6: zero value (Vol. 3C, Page 35-5) */
//  write_msr(IA32_PMC7, ZERO_VALUE, ZERO_VALUE);                               /* writing on: ia32_pmc7: zero value (Vol. 3C, Page 35-5) */